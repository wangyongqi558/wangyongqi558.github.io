---
layout: single
title: "Multi-modal Prompting for Open-vocabulary Video Visual Relationship Detection (AAAI 2024)"
collection: portfolio
teaser: "/images/aaai24_method.png"
author_profile: false
abstract_short: "Adapts CLIP for Open-VidVRD by prompt-tuning on both visual representation and language input sides."
method_short: "Employs learnable continuous and conditional prompts along with vision-guided language prompting."
result_short: "Achieves nearly 10% mAP improvement on the 'Novel' split of the VidVRD dataset."
---

## Abstract
Open-vocabulary Video Visual Relationship Detection (Open-VidVRD) aims to detect subject-predicate-object triplets where some categories are unseen during training. We propose a **Multi-modal Prompting** method to effectively adapt the pre-trained CLIP model for this task. By optimizing prompts on both visual and textual branches, the model gains the ability to generalize to novel relationship categories without extensive retraining.

## Methodology
The framework focuses on two key adaptations of the CLIP architecture:
1.  **Dual-side Prompt Tuning**: We introduce learnable prompt tokens into both the image encoder and text encoder. This allows the model to align video features with semantic descriptions more flexibly.
2.  **Vision-guided Language Prompting**: Instead of static text prompts, we use visual context to guide the generation of language prompts, helping the model "see" the relationship before naming it.
3.  **Spatial-Temporal Context**: Learnable conditional prompts are used to capture the dynamic changes in video frames.

![Methodology](/images/aaai24_method.png)

## Key Results
* **Performance**: Achieved a significant boost in mAP (Mean Average Precision) on the VidVRD benchmark.
* **Zero-shot Generalization**: Showed strong ability to detect relationships like "standing on" or "chasing" even when they were not in the training set.
* **Efficiency**: Outperformed previous state-of-the-art methods while maintaining a more efficient tuning process.
